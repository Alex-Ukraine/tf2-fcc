{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tenth-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPWptVicvSwFHFrS2ZAzAoR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alex-Ukraine/tf2-fcc/blob/master/tenth_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIl28xz2hlop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KwTEasJhnjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_zq78mcjqu3",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "77978ba7-c326-4bb2-e6c3-ee4c05280334"
      },
      "source": [
        "from google.colab import files\n",
        "path_to_file = list(files.upload().keys())[0]"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-99bf12e3-68b0-41d9-810c-eb8603526b81\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-99bf12e3-68b0-41d9-810c-eb8603526b81\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving dan.txt to dan.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP79OKyDhq5w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "14d45d4e-54c2-4405-ecc9-f48d57bdd926"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 461979 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL04nA6XhxQZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "cbd6893b-33db-4556-c9f8-556701558235"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿How many friends does one person need? \r\n",
            "\r\n",
            "Acknowledgements\r\n",
            "\r\n",
            "This volume had its origins in a series of popular science articles that I wrote for New Scientist magazine (mostly between 1994 and 2006) and the Scotsman newspaper (between 2005 and 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjypfzHFhzyo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c981d4d6-2e5f-4d8f-a53c-06a3e7bd46b0"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qleKG4mvh0Vz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBWwyURzh10Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "cfba538e-365c-419e-f691-42ddd53f2030"
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  '\\r':   1,\n",
            "  ' ' :   2,\n",
            "  '!' :   3,\n",
            "  '$' :   4,\n",
            "  '(' :   5,\n",
            "  ')' :   6,\n",
            "  '*' :   7,\n",
            "  '+' :   8,\n",
            "  ',' :   9,\n",
            "  '-' :  10,\n",
            "  '.' :  11,\n",
            "  '/' :  12,\n",
            "  '0' :  13,\n",
            "  '1' :  14,\n",
            "  '2' :  15,\n",
            "  '3' :  16,\n",
            "  '4' :  17,\n",
            "  '5' :  18,\n",
            "  '6' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9xNVS7Th3jh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93fd4745-8157-4a8d-cc9f-daede5cc16e0"
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'\\ufeffHow many fri' ---- characters mapped to int ---- > [96 34 69 77  2 67 55 68 79  2 60 72 63]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnBkMeCjh5O5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b32cc96d-5473-47c4-99fc-77c01230eeb5"
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 50\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿\n",
            "H\n",
            "o\n",
            "w\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrlkIrVNh7BZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "16b85444-947d-4185-8b11-3f58643da2bd"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'\\ufeffHow many friends does one person need? \\r\\n\\r\\nAcknowl'\n",
            "'edgements\\r\\n\\r\\nThis volume had its origins in a serie'\n",
            "'s of popular science articles that I wrote for New '\n",
            "'Scientist magazine (mostly between 1994 and 2006) a'\n",
            "'nd the Scotsman newspaper (between 2005 and 2008). '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GD0sj7Ah-4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjr-SisGh_Wx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3dfeb88a-c8b9-4fe9-fb8c-1aad78d4f88b"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  '\\ufeffHow many friends does one person need? \\r\\n\\r\\nAcknow'\n",
            "Target data: 'How many friends does one person need? \\r\\n\\r\\nAcknowl'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ0J2FK9iBOR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "84ec9815-051a-4b89-b797-ce767ddb15cd"
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "  print(\"Step {:4d}\".format(i))\n",
        "  print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "  print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 96 ('\\ufeff')\n",
            "  expected output: 34 ('H')\n",
            "Step    1\n",
            "  input: 34 ('H')\n",
            "  expected output: 69 ('o')\n",
            "Step    2\n",
            "  input: 69 ('o')\n",
            "  expected output: 77 ('w')\n",
            "Step    3\n",
            "  input: 77 ('w')\n",
            "  expected output: 2 (' ')\n",
            "Step    4\n",
            "  input: 2 (' ')\n",
            "  expected output: 67 ('m')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgxoYJe6iC7B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35b0efc9-d03a-45d8-a754-305edc061af9"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 50), (64, 50)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNxFEPCLiFy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Afi0ya0giIcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToJRjnW0iJDX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "fc4bc0f7-422b-450d-9473-85610d504c1a"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size = len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6J94ixqiMPw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93b4f0a8-2374-40f7-f363-4c505c3afcaa"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 50, 97) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9XJ-vVriMsf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "2f26afee-b38d-4440-ccca-175787e5212a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (64, None, 256)           24832     \n",
            "_________________________________________________________________\n",
            "gru_9 (GRU)                  (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (64, None, 97)            99425     \n",
            "=================================================================\n",
            "Total params: 4,062,561\n",
            "Trainable params: 4,062,561\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAIFE9rDiOYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNLafQk0iQvp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5b690fb2-2ec2-4b99-c5c3-69c07c7b71cf"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([88, 30, 35, 92, 76, 56, 86, 92, 82, 51,  1, 17, 63, 59, 17,  7, 43,\n",
              "       77, 13, 67, 42, 78, 88, 35, 66, 69, 62, 59, 12, 63, 91, 37, 58, 31,\n",
              "       24, 46,  4, 61,  6, 80,  5,  7, 67, 84, 22,  2, 53, 25,  2, 80])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hB93UZjiR8x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6a7fb363-df8a-4b2b-c641-a64c5c93bf62"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'been. We witnessed a genuine scientific revolution'\n",
            "\n",
            "Next Char Predictions: \n",
            " 'ïDIüvbêüàY\\r4ie4*Qw0mPxïIlohe/iöKdE;T$g)z(*mè9 [= z'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDO62mOiiTX6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bbafc0db-bc7b-404c-f801-5801bd712053"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 50, 97)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.5760612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yONfVk7wiUw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv4m0WLqiWAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTkDG5F9iX45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=100"
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37c8q-sMiZ0h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "64d97bd5-59eb-4165-aa64-6213a51408bc"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "141/141 [==============================] - 4s 28ms/step - loss: 2.7494\n",
            "Epoch 2/100\n",
            "141/141 [==============================] - 4s 28ms/step - loss: 2.1241\n",
            "Epoch 3/100\n",
            "141/141 [==============================] - 4s 28ms/step - loss: 1.8689\n",
            "Epoch 4/100\n",
            "141/141 [==============================] - 4s 28ms/step - loss: 1.6676\n",
            "Epoch 5/100\n",
            "141/141 [==============================] - 4s 28ms/step - loss: 1.5346\n",
            "Epoch 6/100\n",
            "141/141 [==============================] - 4s 28ms/step - loss: 1.4423\n",
            "Epoch 7/100\n",
            "141/141 [==============================] - 4s 28ms/step - loss: 1.3689\n",
            "Epoch 8/100\n",
            "141/141 [==============================] - 4s 28ms/step - loss: 1.3109\n",
            "Epoch 9/100\n",
            "141/141 [==============================] - 4s 29ms/step - loss: 1.2576\n",
            "Epoch 10/100\n",
            "141/141 [==============================] - 4s 28ms/step - loss: 1.2088\n",
            "Epoch 11/100\n",
            "141/141 [==============================] - 4s 28ms/step - loss: 1.1593\n",
            "Epoch 12/100\n",
            "141/141 [==============================] - 4s 29ms/step - loss: 1.1109\n",
            "Epoch 13/100\n",
            "141/141 [==============================] - 4s 29ms/step - loss: 1.0655\n",
            "Epoch 14/100\n",
            "141/141 [==============================] - 4s 29ms/step - loss: 1.0192\n",
            "Epoch 15/100\n",
            "141/141 [==============================] - 4s 29ms/step - loss: 0.9728\n",
            "Epoch 16/100\n",
            "141/141 [==============================] - 4s 29ms/step - loss: 0.9251\n",
            "Epoch 17/100\n",
            "141/141 [==============================] - 4s 29ms/step - loss: 0.8830\n",
            "Epoch 18/100\n",
            "141/141 [==============================] - 4s 29ms/step - loss: 0.8403\n",
            "Epoch 19/100\n",
            "141/141 [==============================] - 4s 29ms/step - loss: 0.8042\n",
            "Epoch 20/100\n",
            "141/141 [==============================] - 4s 29ms/step - loss: 0.7659\n",
            "Epoch 21/100\n",
            "141/141 [==============================] - 4s 29ms/step - loss: 0.7347\n",
            "Epoch 22/100\n",
            "141/141 [==============================] - 4s 29ms/step - loss: 0.7075\n",
            "Epoch 23/100\n",
            "141/141 [==============================] - 4s 29ms/step - loss: 0.6827\n",
            "Epoch 24/100\n",
            "141/141 [==============================] - 4s 29ms/step - loss: 0.6591\n",
            "Epoch 25/100\n",
            "141/141 [==============================] - 4s 29ms/step - loss: 0.6380\n",
            "Epoch 26/100\n",
            "141/141 [==============================] - 4s 29ms/step - loss: 0.6253\n",
            "Epoch 27/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.6079\n",
            "Epoch 28/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5965\n",
            "Epoch 29/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5865\n",
            "Epoch 30/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5766\n",
            "Epoch 31/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5675\n",
            "Epoch 32/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5639\n",
            "Epoch 33/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5564\n",
            "Epoch 34/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5505\n",
            "Epoch 35/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5464\n",
            "Epoch 36/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5431\n",
            "Epoch 37/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5398\n",
            "Epoch 38/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5382\n",
            "Epoch 39/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5351\n",
            "Epoch 40/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5321\n",
            "Epoch 41/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5313\n",
            "Epoch 42/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5282\n",
            "Epoch 43/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5279\n",
            "Epoch 44/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5248\n",
            "Epoch 45/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5253\n",
            "Epoch 46/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5271\n",
            "Epoch 47/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5262\n",
            "Epoch 48/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5265\n",
            "Epoch 49/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5269\n",
            "Epoch 50/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5282\n",
            "Epoch 51/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5272\n",
            "Epoch 52/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5283\n",
            "Epoch 53/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5318\n",
            "Epoch 54/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.5349\n",
            "Epoch 55/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5341\n",
            "Epoch 56/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5387\n",
            "Epoch 57/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5381\n",
            "Epoch 58/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.5417\n",
            "Epoch 59/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5395\n",
            "Epoch 60/100\n",
            "141/141 [==============================] - 4s 30ms/step - loss: 0.5439\n",
            "Epoch 61/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.5490\n",
            "Epoch 62/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.5473\n",
            "Epoch 63/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.5501\n",
            "Epoch 64/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.5562\n",
            "Epoch 65/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.5632\n",
            "Epoch 66/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.5664\n",
            "Epoch 67/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.5686\n",
            "Epoch 68/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.5764\n",
            "Epoch 69/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.5811\n",
            "Epoch 70/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.5889\n",
            "Epoch 71/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.5947\n",
            "Epoch 72/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.5984\n",
            "Epoch 73/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.6077\n",
            "Epoch 74/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.6148\n",
            "Epoch 75/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.6224\n",
            "Epoch 76/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.6282\n",
            "Epoch 77/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.6370\n",
            "Epoch 78/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.6505\n",
            "Epoch 79/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.6621\n",
            "Epoch 80/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.6712\n",
            "Epoch 81/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.6801\n",
            "Epoch 82/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.6900\n",
            "Epoch 83/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.7033\n",
            "Epoch 84/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.7191\n",
            "Epoch 85/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.7321\n",
            "Epoch 86/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.7555\n",
            "Epoch 87/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.7718\n",
            "Epoch 88/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.7924\n",
            "Epoch 89/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.8108\n",
            "Epoch 90/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.8242\n",
            "Epoch 91/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.8382\n",
            "Epoch 92/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.8584\n",
            "Epoch 93/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.8735\n",
            "Epoch 94/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.9032\n",
            "Epoch 95/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.9282\n",
            "Epoch 96/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.9410\n",
            "Epoch 97/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 0.9631\n",
            "Epoch 98/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 1.0087\n",
            "Epoch 99/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 1.0214\n",
            "Epoch 100/100\n",
            "141/141 [==============================] - 4s 31ms/step - loss: 1.0293\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFUTgVaMibNi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "74144af6-35d2-4077-ff05-7fc41dbffc31"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_100'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq-lBDAuic7b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "e68fbcc4-8d3f-4a62-82cb-5939570b1f86"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lxRvTcpifq6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "2a5f995f-c85f-4aae-8650-56ff91ccb1e9"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_10 (Embedding)     (1, None, 256)            24832     \n",
            "_________________________________________________________________\n",
            "gru_10 (GRU)                 (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (1, None, 97)             99425     \n",
            "=================================================================\n",
            "Total params: 4,062,561\n",
            "Trainable params: 4,062,561\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqgviPFqihFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # We pass the predicted character as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3agBGKTzii5x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "e0c3537a-fde9-4f40-ed68-ece9a03960e3"
      },
      "source": [
        "print(generate_text(model, start_string=u\"was\"))"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "was brain\r\n",
            "\r\n",
            "so complete Darwin’. \r\n",
            "\r\n",
            "Hence, with the ideclimitive by good for you\r\n",
            "\r\n",
            "What tec societion is transmits in a harvest back to fight. They alive mitionary because their possetion, to pussionally, God wants casefay?’ was only forced the free birded local backqup or out fasignuses can rescues of spearch. It cassior of the oldest don’t tell-unking, we know that of co-of which placts that is thquirden as marterable burial grooming. We bs the female life is alllue as orromance they troughly of treen to the same smale coble could really by exception of 150 because they scholar and so named Braskfix, they would expect them alwaysally structured in an onalog. Nearly been lifed are distrefect thats were didebabout the qualtations of the reason is viller (fictions that men than frequency of our community, the but rulkerfor martoric colonishes with sught stage by vision of comeology. Foot ever from the greates a big brains true: even a fast size of Licting breath of young chest the first\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}